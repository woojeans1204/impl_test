import torch
import tiktoken
from src.model import GPT

# 1. ì„¤ì • (í•™ìŠµ ëë‚œ ëª¨ë¸ ê²½ë¡œ)
CHECKPOINT_PATH = "results/nanogpt_alpaca_finetune/checkpoints/last.pth"
device = 'cuda' if torch.cuda.is_available() else 'cpu'

# 2. ëª¨ë¸ ë¡œë“œ (ì˜µì…˜ ì£¼ì˜!)
print(f"Loading model from {CHECKPOINT_PATH}...")
checkpoint = torch.load(CHECKPOINT_PATH, map_location=device, weights_only=False)
model = GPT(checkpoint['config'])
model.load_state_dict(checkpoint['model_state'])
model.to(device)
model.eval()

# 3. í† í¬ë‚˜ì´ì €
enc = tiktoken.get_encoding("gpt2")

def generate_response(user_input):
    # [í•µì‹¬] Alpaca í”„ë¡¬í”„íŠ¸ í¬ë§· ì”Œìš°ê¸°
    prompt = f"""Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:
{user_input}

### Response:
"""
    # ì¸ì½”ë”© & GPU ì „ì†¡
    input_ids = torch.tensor(enc.encode(prompt), dtype=torch.long, device=device).unsqueeze(0)

    # ìƒì„± (ë‹µë³€ ë¶€ë¶„ë§Œ)
    with torch.no_grad():
        output_ids = model.generate(input_ids, max_new_tokens=200, temperature=0.7)
        # í”„ë¡¬í”„íŠ¸ ê¸¸ì´ë¥¼ ì œì™¸í•˜ê³  ë‹µë³€ë§Œ ì˜ë¼ëƒ„
        response_ids = output_ids[0].tolist()[len(input_ids[0]):]
        response_text = enc.decode(response_ids)
        
    return response_text.strip()

# 4. ì±„íŒ… ë£¨í”„
print("\n" + "="*30)
print("ğŸ¤– Alpaca-NanoGPT Chatbot")
print("quitë¥¼ ì…ë ¥í•˜ë©´ ì¢…ë£Œí•©ë‹ˆë‹¤.")
print("="*30 + "\n")

while True:
    user_input = input("User: ")
    if user_input.lower() == "quit":
        break
    
    response = generate_response(user_input)
    print(f"Bot : {response}\n")