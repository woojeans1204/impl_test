# [시스템 설정]
system:
  experiment_name: "gpt2_large_alpaca_finetune"  # 결과 저장 폴더명
  seed: 1337

# [데이터 설정]
data:
  data_dir: "data/alpaca" # 기존 Alpaca 데이터 그대로 사용 가능

# [모델 설정] - GPT-2 Large 공식 사양
model:
  block_size: 1024
  vocab_size: 50257
  n_layer: 36        # Small(12) -> Large(36)
  n_head: 20         # Small(12) -> Large(20)
  n_embd: 1280       # Small(768) -> Large(1280)
  dropout: 0.1
  bias: true

# [학습 설정]
train:
  # [중요] 미리 복사해둔 Large 베이스 모델을 불러오기 위해 에포크 1부터 시작하게 세팅하세요.
  epochs: 100
  save_interval: 10
  
  # [메모리 주의] V100 16GB/32GB 환경에서 Large 모델은 메모리를 많이 먹습니다.
  # 8로 하면 100% 터집니다. 1~2 정도로 낮춰서 시작하세요.
  batch_size: 2      
  
  # 모델이 클수록 가중치가 쉽게 틀어질 수 있어 더 낮은 학습률을 권장합니다.
  learning_rate: 5e-6 
  min_lr: 1e-6
  weight_decay: 0.01
  grad_clip: 1.0
  beta1: 0.9
  beta2: 0.99