í˜„ì¬ê¹Œì§€ ì§„í–‰ëœ **Pre-training(ì‚¬ì „ í•™ìŠµ)** ë‹¨ê³„ì— ëŒ€í•œ `README.md` ì´ˆì•ˆì…ë‹ˆë‹¤.

í”„ë¡œì íŠ¸ì˜ ë°œì „ ê³¼ì •(Shakespeare â†’ TinyStories â†’ FineWeb-Edu)ê³¼ í˜„ì¬ êµ¬ì¶•ëœ íŒŒì¼ ì‹œìŠ¤í…œ êµ¬ì¡°ë¥¼ ëª…í™•í•˜ê²Œ ë°˜ì˜í•˜ì—¬ ì‘ì„±í–ˆìŠµë‹ˆë‹¤. ì´ íŒŒì¼ì„ í”„ë¡œì íŠ¸ ìµœìƒë‹¨(`readme.md`)ì— ì €ì¥í•˜ì‹œë©´ ë©ë‹ˆë‹¤.

---

# ğŸ§  Custom NanoGPT: Pre-training Phase

ì´ í”„ë¡œì íŠ¸ëŠ” GPT(Generative Pre-trained Transformer) ëª¨ë¸ì„ ë°”ë‹¥ë¶€í„° êµ¬í˜„í•˜ê³ , ë‹¨ê³„ë³„ ë°ì´í„°ì…‹ì„ í†µí•´ ì‚¬ì „ í•™ìŠµ(Pre-training)ì„ ìˆ˜í–‰í•œ ê¸°ë¡ì…ë‹ˆë‹¤. **Andrej Karpathyì˜ NanoGPT**ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ë©°, ë‹¤ì¤‘ GPU í™˜ê²½(`Accelerate`)ê³¼ ëŒ€ìš©ëŸ‰ ë°ì´í„°ì…‹ ì²˜ë¦¬ì— ìµœì í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

## ğŸ“‚ Project Structure

í˜„ì¬ í”„ë¡œì íŠ¸ì˜ ë””ë ‰í† ë¦¬ êµ¬ì¡°ì…ë‹ˆë‹¤.

```text
â”œâ”€â”€ configs/               # ì‹¤í—˜ë³„ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì • (YAML)
â”‚   â””â”€â”€ exp260218/         # 2026-02-18 ì‹¤í—˜êµ°
â”‚       â”œâ”€â”€ base.yaml      # ê¸°ë³¸ ì„¤ì •
â”‚       â”œâ”€â”€ stories.yaml   # TinyStoriesìš© ì„¤ì •
â”‚       â””â”€â”€ fineweb.yaml   # FineWeb-Eduìš© ì„¤ì •
â”œâ”€â”€ data/                  # í•™ìŠµ ë°ì´í„° ë° ì „ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸
â”‚   â”œâ”€â”€ shakespeare/       # (Step 1) ë¬¸ì/ë‹¨ì–´ ë‹¨ìœ„ ê¸°ì´ˆ í•™ìŠµ
â”‚   â”œâ”€â”€ tinystories/       # (Step 2) ê¸°ì´ˆ ë¬¸ë²• ë° ì´ì•¼ê¸° êµ¬ì¡° í•™ìŠµ
â”‚   â””â”€â”€ fineweb/           # (Step 3) ì¼ë°˜ ìƒì‹ ë° ë…¼ë¦¬ í•™ìŠµ (Current)
â”œâ”€â”€ results/               # í•™ìŠµ ê²°ê³¼ë¬¼ (ì²´í¬í¬ì¸íŠ¸, ë¡œê·¸, ìƒ˜í”Œ)
â”‚   â”œâ”€â”€ shakespeare_gpt_v1
â”‚   â”œâ”€â”€ tinystories_gpt_v1
â”‚   â””â”€â”€ fineweb_gpt_v2     # í˜„ì¬ ë©”ì¸ ì‹¤í—˜ ê²°ê³¼
â”œâ”€â”€ src/                   # í•µì‹¬ ì†ŒìŠ¤ ì½”ë“œ
â”‚   â”œâ”€â”€ model.py           # GPT ëª¨ë¸ ì•„í‚¤í…ì²˜ (PyTorch)
â”‚   â”œâ”€â”€ trainer.py         # í•™ìŠµ ë£¨í”„, ì²´í¬í¬ì¸íŠ¸, ìƒ˜í”Œë§ ë¡œì§
â”‚   â””â”€â”€ dataset.py         # ëŒ€ìš©ëŸ‰ ë°ì´í„° ë¡œë” (Memory mapping)
â”œâ”€â”€ train.py               # ë‹¨ì¼ ì‹¤í—˜ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ run_manager.py         # ì‹¤í—˜ ìŠ¤ì¼€ì¤„ëŸ¬ (ìˆœì°¨ì  ì‹¤í—˜ ì‹¤í–‰)
â”œâ”€â”€ inference.py           # í…ìŠ¤íŠ¸ ìƒì„±(Inference) ìŠ¤í¬ë¦½íŠ¸
â””â”€â”€ experiment_list.conf   # run_managerê°€ ì‹¤í–‰í•  ì‹¤í—˜ ëª©ë¡

```

---

## ğŸš€ Quick Start

### 1. Dependencies

í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤.

```bash
pip install torch numpy transformers datasets tiktoken accelerate pyyaml tqdm

```

### 2. Data Preparation

ë°ì´í„°ì…‹ í¬ê¸°ì— ë”°ë¼ ë‹¨ê³„ë³„ë¡œ ì „ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤. ì „ì²˜ë¦¬ ê²°ê³¼ëŠ” `.bin` (uint16) í˜•íƒœë¡œ ì €ì¥ë©ë‹ˆë‹¤.

**Step 1: Shakespeare (Char/Word level)**
ê°€ë²¼ìš´ í…ŒìŠ¤íŠ¸ìš©ì…ë‹ˆë‹¤.

```bash
python data/shakespeare/prepare_gptT.py

```

**Step 2: TinyStories (Narrative)**
ë¬¸ë²•ê³¼ ê¸°ì´ˆì ì¸ ìŠ¤í† ë¦¬í…”ë§ì„ í•™ìŠµí•©ë‹ˆë‹¤.

```bash
python data/tinystories/prepare_tinystories.py

```

**Step 3: FineWeb-Edu (Knowledge & Reasoning)**
ì›¹ìƒì˜ ê³ í’ˆì§ˆ êµìœ¡ ë°ì´í„°ë¥¼ í•™ìŠµí•©ë‹ˆë‹¤. (í˜„ì¬ ë©”ì¸)

```bash
python data/fineweb/prepare_fineweb.py

```

---

## âš™ï¸ Configuration

ì‹¤í—˜ ì„¤ì •ì€ `configs/` í´ë” ë‚´ì˜ YAML íŒŒì¼ë¡œ ê´€ë¦¬í•©ë‹ˆë‹¤.

| ì„¤ì • íŒŒì¼ | ìš©ë„ | ì£¼ìš” íŠ¹ì§• |
| --- | --- | --- |
| `stories.yaml` | TinyStories í•™ìŠµ | ì‘ì€ ëª¨ë¸, ë¹ ë¥¸ ìˆ˜ë ´ í™•ì¸ìš© |
| `fineweb.yaml` | FineWeb-Edu í•™ìŠµ | **Main Model**. `n_layer=12`, `n_head=12`, `n_embd=768` (GPT-2 Smallê¸‰) |

---

## ğŸ”¥ Training

### ë‹¨ì¼ ì‹¤í—˜ ì‹¤í–‰ (`train.py`)

íŠ¹ì • ì„¤ì • íŒŒì¼ í•˜ë‚˜ë¡œ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.

```bash
accelerate launch train.py --config configs/exp260218/fineweb.yaml

```

### ì‹¤í—˜ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹¤í–‰ (`run_manager.py`)

ì—¬ëŸ¬ ì‹¤í—˜ì„ ìˆœì°¨ì ìœ¼ë¡œ ëŒë ¤ì•¼ í•  ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤. `experiment_list.conf`ì— ë“±ë¡ëœ YAML íŒŒì¼ë“¤ì„ ì°¨ë¡€ëŒ€ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.

```bash
# 1. ì‹¤í–‰í•  ë¦¬ìŠ¤íŠ¸ í™•ì¸
cat experiment_list.conf
# (ì˜ˆì‹œ ë‚´ìš©)
# stories.yaml
# fineweb.yaml

# 2. ë§¤ë‹ˆì € ì‹¤í–‰
python run_manager.py

```

**Key Features:**

* **Accelerate Integration:** ë‹¨ì¼ GPU ë° ë‹¤ì¤‘ GPU í™˜ê²½ ìë™ ëŒ€ì‘.
* **Resume Capability:** ì¤‘ë‹¨ëœ í•™ìŠµ ì‹œ `checkpoints/last.pth`ë¥¼ ê°ì§€í•˜ì—¬ ìë™ ì¬ê°œ.
* **Random Sampling:** ëŒ€ìš©ëŸ‰ ë°ì´í„°ì…‹(FineWeb)ì˜ ê²½ìš°, ì—í¬í¬ë§ˆë‹¤ ë°ì´í„°ë¥¼ ëœë¤í•˜ê²Œ ìƒ˜í”Œë§í•˜ì—¬ íš¨ìœ¨ì ìœ¼ë¡œ í•™ìŠµ.

---

## ğŸ§ª Results & Monitoring

í•™ìŠµ ê²°ê³¼ëŠ” `results/{ì‹¤í—˜ëª…}/` ì•„ë˜ì— ì €ì¥ë©ë‹ˆë‹¤.

* **checkpoints/**: `last.pth` (ìµœì‹ ), `ckpt_epoch_*.pth` (ì£¼ê¸°ì  ì €ì¥)
* **logs/**: TensorBoard ë¡œê·¸. `tensorboard --logdir results`ë¡œ í™•ì¸ ê°€ëŠ¥.
* **samples/**: í•™ìŠµ ì¤‘ê°„ì— ìƒì„±ëœ í…ìŠ¤íŠ¸ ìƒ˜í”Œ.
* **Fixed Prompts:** ëª¨ë¸ì˜ ë°œì „ ê³¼ì •ì„ ë¹„êµí•˜ê¸° ìœ„í•´ ë§¤ ì—í¬í¬ë§ˆë‹¤ 4ê°œì˜ ê³ ì •ëœ ì§ˆë¬¸("AIë€ ë¬´ì—‡ì¸ê°€", "1+1ì€" ë“±)ì— ëŒ€í•œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.



**í˜„ì¬ ì§„í–‰ ìƒí™© (Example):**

* `shakespeare_gpt_v1`: ì´ˆê¸° êµ¬ì¡° ê²€ì¦ ì™„ë£Œ.
* `fineweb_gpt_v2`: **100 Epoch ë‹¬ì„±**. ë¬¸ë²• ì™„ì„± ë° ê¸°ë³¸ ìƒì‹ ì¶”ë¡  ê°€ëŠ¥ ë‹¨ê³„.

---

## ğŸ’¬ Inference

í•™ìŠµëœ ëª¨ë¸(`ckpt`)ì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

```bash
python inference.py

```

* `inference.py` ë‚´ë¶€ì˜ `CHECKPOINT_PATH`ë¥¼ ì›í•˜ëŠ” ëª¨ë¸ ê²½ë¡œ(ì˜ˆ: `results/fineweb_gpt_v2/checkpoints/last.pth`)ë¡œ ìˆ˜ì •í•˜ì—¬ ì‚¬ìš©í•˜ì„¸ìš”.
* GPT-2 `tiktoken`ì„ ì‚¬ìš©í•˜ì—¬ ìì—°ìŠ¤ëŸ¬ìš´ í† í¬ë‚˜ì´ì§•ì„ ì§€ì›í•©ë‹ˆë‹¤.

---

## ğŸ“ Note

* **Hardware:** V100 GPU í™˜ê²½ì—ì„œ í…ŒìŠ¤íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.
* **Dataset:** `fineweb/train.bin`ì€ ìš©ëŸ‰ ë¬¸ì œë¡œ ìƒ˜í”Œë§ëœ ë°ì´í„°(ì•½ 1%~10%)ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
* **Next Step:** í˜„ì¬ Pre-trainingì´ ì™„ë£Œë˜ì—ˆìœ¼ë©°, Instruction Tuning(Alpaca ë°ì´í„°ì…‹ ë“±)ì„ í†µí•œ Fine-tuning ë‹¨ê³„ë¡œ ë„˜ì–´ê°ˆ ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤.