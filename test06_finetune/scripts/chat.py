import torch
import tiktoken
import os
import sys
current_dir = os.path.dirname(os.path.abspath(__file__)) # scripts í´ë”
parent_dir = os.path.dirname(current_dir)               # ìƒìœ„ í´ë” (test6)
sys.path.append(parent_dir)
# src í´ë” ì¸ì‹ì„ ìœ„í•´ ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from src.model import GPT, GPTConfig 

# ==========================================
# 1. ì„¤ì •
# ==========================================
CHECKPOINT_PATH = "../results/gpt2_large_alpaca_finetune/checkpoints/last.pth"
DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'

# ==========================================
# 2. ëª¨ë¸ ë¡œë“œ ë° ì„¤ì • ë³€í™˜ (í•µì‹¬!)
# ==========================================
print(f">>> Loading model from {CHECKPOINT_PATH}...")
if not os.path.exists(CHECKPOINT_PATH):
    print(f"Error: ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤ ({CHECKPOINT_PATH})")
    exit()

# weights_only=Falseë¡œ ë¡œë“œ (ì»¤ìŠ¤í…€ í´ë˜ìŠ¤/ë”•ì…”ë„ˆë¦¬ í˜¸í™˜)
checkpoint = torch.load(CHECKPOINT_PATH, map_location=DEVICE, weights_only=False)
raw_config = checkpoint['config']

# [Config ë³€í™˜ ë¡œì§] ë”•ì…”ë„ˆë¦¬ë¥¼ GPTConfig ê°ì²´ë¡œ ì•ˆì „í•˜ê²Œ ë³€í™˜
if isinstance(raw_config, dict):
    # ë§Œì•½ ì „ì²´ YAML ì„¤ì •(system, train, model ë“±)ì´ ë“¤ì–´ìˆë‹¤ë©´ 'model'ë§Œ ì¶”ì¶œ
    if 'model' in raw_config:
        model_args = raw_config['model']
    else:
        model_args = raw_config
    
    # ë”•ì…”ë„ˆë¦¬ë¥¼ í’€ì–´ì„œ ê°ì²´ ìƒì„±
    config = GPTConfig(**model_args)
else:
    # ì´ë¯¸ ê°ì²´ë¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    config = raw_config

print(f">>> Model Config Loaded: n_layer={config.n_layer}, n_head={config.n_head}, n_embd={config.n_embd}")

# ëª¨ë¸ ì´ˆê¸°í™”
model = GPT(config)
model.load_state_dict(checkpoint['model_state'])
model.to(DEVICE)
model.eval()

# ==========================================
# 3. ì¸í¼ëŸ°ìŠ¤ í•¨ìˆ˜
# ==========================================
enc = tiktoken.get_encoding("gpt2")

def generate_response(user_input):
    # Alpaca í¬ë§· ì ìš©
    prompt = f"""Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:
{user_input}

### Response:
"""
    # ì…ë ¥ ì¸ì½”ë”©
    input_ids = torch.tensor(enc.encode(prompt), dtype=torch.long, device=DEVICE).unsqueeze(0)

    with torch.no_grad():
        # max_new_tokensë¥¼ ë„‰ë„‰íˆ ì¤Œ (Stop Tokenìœ¼ë¡œ ìë¥¼ ì˜ˆì •)
        output_ids = model.generate(input_ids, max_new_tokens=512, temperature=0.7)
        
        # ì…ë ¥ í”„ë¡¬í”„íŠ¸ ë¶€ë¶„ì€ ì œì™¸í•˜ê³  ë‹µë³€ë§Œ ì¶”ì¶œ
        response_ids = output_ids[0].tolist()[len(input_ids[0]):]
        response_text = enc.decode(response_ids)
        
    # [í•µì‹¬] <|endoftext|> í† í°ì´ ë‚˜ì˜¤ë©´ ê·¸ ë’¤ëŠ” ì‹¹ë‘‘ ìë¥´ê¸°
    if "<|endoftext|>" in response_text:
        response_text = response_text.split("<|endoftext|>")[0]
        
    return response_text.strip()

# ==========================================
# 4. ì±„íŒ… ë£¨í”„
# ==========================================
print("\n" + "="*40)
print("ğŸ¤– Alpaca-NanoGPT Chatbot is Ready!")
print("   (ì¢…ë£Œí•˜ë ¤ë©´ 'quit' ì…ë ¥)")
print("="*40 + "\n")

while True:
    try:
        user_input = input("User: ")
        if user_input.lower() in ["quit", "exit"]:
            print("Bye!")
            break
        
        if not user_input.strip():
            continue

        print("Thinking...", end="\r")
        response = generate_response(user_input)
        
        # ì´ì „ ì¶œë ¥ ë®ì–´ì“°ê³  ê²°ê³¼ ì¶œë ¥
        print(f"Bot : {response}\n")
        print("-" * 40)
        
    except KeyboardInterrupt:
        print("\nInterrupted. Bye!")
        break
    except Exception as e:
        print(f"\nError: {e}")